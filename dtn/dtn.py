"""
True_DTN: ç†è®ºä¸‹ç•Œ - æ— é¢„è®­ç»ƒç›´æ¥è®­ç»ƒ
æ¯ä¸ªepisodeç‹¬ç«‹åˆå§‹åŒ–+è®­ç»ƒï¼Œæ¨¡æ‹Ÿæœ€åæƒ…å†µ

ç†è®ºå®šä½ï¼šè¯„ä¼°åœ¨æç«¯æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹ï¼Œæ— å…ˆéªŒçŸ¥è¯†çš„å­¦ä¹ èƒ½åŠ›
è®¾è®¡ç›®æ ‡ï¼šä¸ºå…¶ä»–æ–¹æ³•æä¾›æ€§èƒ½æ¯”è¾ƒçš„åŸºå‡†çº¿
"""
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Tuple, Dict, Any
import time
import numpy as np

from methods.base_trainer import BaseTrainer, EpisodeMetrics
from models.networks import CNN1dEncoder, LinearClassifier, init_weights
from data.pu_loader import FinetuneTask, get_finetune_loader


class DTNTrainer(BaseTrainer):
    """DTN: æ— é¢„è®­ç»ƒçš„ç†è®ºä¸‹ç•Œ"""

    def __init__(self, config: Any):
        super().__init__('DTN', config)
        self.feature_dim = config.model.feature_dim
        self.learning_rate = config.training.learning_rate

        # è®­ç»ƒè½®æ•°é…ç½®
        self.train_episode = getattr(
            config.training, 'dtn_train_episode',
            config.training.finetune_episode
        )

        self.test_episode = config.training.test_episode
        self.batch_size_test = config.training.batch_size_test

        # å…³é”®ï¼šè·Ÿè¸ªå½“å‰æ˜¯ç¬¬å‡ æ¬¡ run
        self._current_run_id = 0

    def train(self, metatrain_data: list) -> Tuple[None, float]:
        """é¢„è®­ç»ƒé˜¶æ®µ - å®Œå…¨è·³è¿‡

    å…³é”®è®¾è®¡ï¼šä¸åˆ©ç”¨ä»»ä½•æºåŸŸçŸ¥è¯†
    - è¿”å›Noneè¡¨ç¤ºæ— é¢„è®­ç»ƒæ¨¡å‹
    - è®­ç»ƒæ—¶é—´ä¸º0
    - æ¨¡æ‹ŸçœŸå®åœºæ™¯ä¸­æ— æ³•è·å¾—ç›¸å…³æ•°æ®çš„æƒ…å†µ
    """
        self._current_run_id += 1
        self.logger.warning("DTN: No pre-training (theoretical lower bound)")
        return None, 0.0

    def test(self, model: None, metatest_data: list) -> Dict[str, Any]:
        """
        æµ‹è¯•é˜¶æ®µï¼šæ¯ä¸ªshoté…ç½®æµ‹è¯•100ä¸ªepisode

        å…³é”®ä¿®æ”¹ï¼š
        - åŸºäº _current_run_id ç”Ÿæˆç‹¬ç«‹çš„ run_seed
        - æ¯æ¬¡ run ä½¿ç”¨ä¸åŒçš„ä»»åŠ¡é‡‡æ ·ç§å­

        Args:
            model: å ä½ç¬¦ï¼ˆå®é™…ä¸ºNoneï¼‰
            metatest_data: æµ‹è¯•ç±»åˆ«æ•°æ®

        Returns:
            results: å„shoté…ç½®çš„æµ‹è¯•ç»“æœ
        """
        # å…³é”®ï¼šåŸºäº run_id ç”Ÿæˆç‹¬ç«‹çš„ç§å­
        run_seed = self.config.training.random_seed + (self._current_run_id - 1) * 100000

        self.logger.info(f"Using run_seed={run_seed} for task sampling")

        results = {}

        for shot in self.config.training.shot_configs:
            self.logger.info(f"Testing {shot}-shot (training from scratch)...")

            # ä¼ é€’ run_seed åˆ°æµ‹è¯•å‡½æ•°
            shot_acc = self._test_single_shot(metatest_data, shot, run_seed)
            results[f'{shot}shot'] = shot_acc

            self.logger.info(
                f"{shot}-shot: Mean={shot_acc['mean']:.4f} Â± {shot_acc['std']:.4f}"
            )

        return results

    def _test_single_shot(self, metatest_data: list, shot: int,
                          run_seed: int) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªshoté…ç½® - å®é™…åŒ…å«è®­ç»ƒè¿‡ç¨‹

    å…³é”®ç‰¹æ€§ï¼šæ¯ä¸ªepisodeå®Œå…¨ç‹¬ç«‹
    - ä¸åŒçš„éšæœºåˆå§‹åŒ–
    - ä¸åŒçš„ä»»åŠ¡é‡‡æ ·
    - ä¸åŒçš„è®­ç»ƒè¿‡ç¨‹
    """
        metrics = EpisodeMetrics()

        for episode in range(self.test_episode):
            # å…³é”®ï¼šæ¯ä¸ª episode çš„ç§å­ç”± run_seed å’Œ episode_id å…±åŒå†³å®š
            episode_seed = run_seed + episode * 1000

            # è®¾ç½®éšæœºç§å­ï¼ˆå½±å“ä»»åŠ¡é‡‡æ ·ï¼‰
            torch.manual_seed(episode_seed)
            np.random.seed(episode_seed)

            # åˆ›å»ºä»»åŠ¡ï¼ˆä¸åŒ run çš„ç›¸åŒ episode_id ä¼šå¾—åˆ°ä¸åŒçš„ä»»åŠ¡ï¼‰
            task = FinetuneTask(
                metatest_data,
                support_num=shot,
                seed=episode_seed
            )

            # æ•°æ®åŠ è½½å™¨
            support_loader = get_finetune_loader(
                task,
                batch_size=len(task.support_files),
                split='support',
                shuffle=True,
                data_type=self.config.data.data_type
            )

            query_loader = get_finetune_loader(
                task,
                batch_size=self.batch_size_test,
                split='query',
                shuffle=False,
                data_type=self.config.data.data_type
            )

            # ğŸ¯ æ ¸å¿ƒå·®å¼‚ç‚¹ï¼šæ¯ä¸ªepisodeä»å¤´è®­ç»ƒ
            accuracy = self._train_and_evaluate(
                support_loader, query_loader,
                len(metatest_data), episode_seed
            )

            metrics.update(accuracy)

            # å®šæœŸè¾“å‡ºè¿›åº¦
            if (episode + 1) % 20 == 0:
                self.logger.info(
                    f"Test Episode {episode + 1}/{self.test_episode} - "
                    f"Acc: {accuracy:.4f}"
                )

        return metrics.compute()

    def _train_and_evaluate(self, support_loader, query_loader,
                            num_classes: int, episode_seed: int) -> float:
        """
        å•ä¸ªepisodeçš„è®­ç»ƒ+è¯„ä¼°æµç¨‹

        å…³é”®ä¿®æ”¹ï¼š
        - ä½¿ç”¨ episode_seed åˆå§‹åŒ–æ¨¡å‹æƒé‡
        - ç¡®ä¿ä¸åŒ run çš„ç›¸åŒ episode_id äº§ç”Ÿä¸åŒçš„æ¨¡å‹åˆå§‹åŒ–

        Args:
            support_loader: æ”¯æŒé›†æ•°æ®
            query_loader: æŸ¥è¯¢é›†æ•°æ®
            num_classes: ç±»åˆ«æ•°
            episode_seed: episode éšæœºç§å­

        Returns:
            accuracy: æŸ¥è¯¢é›†å‡†ç¡®ç‡
        """
        # 1. è®¾ç½®éšæœºç§å­ï¼ˆå½±å“æ¨¡å‹åˆå§‹åŒ–ï¼‰
        torch.manual_seed(episode_seed)
        np.random.seed(episode_seed)

        # 2. éšæœºåˆå§‹åŒ–æ¨¡å‹
        feature_encoder = CNN1dEncoder(
            feature_dim=self.feature_dim,
            flatten=True
        ).to(self.device)

        classifier = LinearClassifier(
            input_dim=self.feature_dim * 25,
            num_classes=num_classes
        ).to(self.device)

        init_weights(feature_encoder)
        init_weights(classifier)

        # 3. ä¼˜åŒ–å™¨ - åŒæ—¶ä¼˜åŒ–ç¼–ç å™¨å’Œåˆ†ç±»å™¨
        optimizer = optim.Adam(
            list(feature_encoder.parameters()) + list(classifier.parameters()),
            lr=self.learning_rate
        )
        criterion = nn.CrossEntropyLoss()

        # 4. è®­ç»ƒå¾ªç¯ - åœ¨å°‘é‡æ”¯æŒé›†æ ·æœ¬ä¸Šè®­ç»ƒ
        feature_encoder.train()
        classifier.train()

        for epoch in range(self.train_episode):
            for batch_x, batch_y in support_loader:
                batch_x, batch_y = self._to_device(batch_x, batch_y)

                # å‰å‘ä¼ æ’­
                features = feature_encoder(batch_x)
                logits = classifier(features)
                loss = criterion(logits, batch_y)

                # åå‘ä¼ æ’­ - æ›´æ–°æ‰€æœ‰å‚æ•°
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        # 5. è¯„ä¼° - åœ¨æŸ¥è¯¢é›†ä¸Šæµ‹è¯•
        feature_encoder.eval()
        classifier.eval()

        correct = 0
        total = 0

        with torch.no_grad():
            for batch_x, batch_y in query_loader:
                batch_x, batch_y = self._to_device(batch_x, batch_y)

                features = feature_encoder(batch_x)
                logits = classifier(features)
                pred = torch.argmax(logits, dim=1)

                correct += (pred == batch_y).sum().item()
                total += batch_y.size(0)

        return correct / total

