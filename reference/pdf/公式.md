# 第一部分：基于知识图谱和数据积累策略的滚动轴承故障诊断
**(Rolling Bearing Fault Diagnosis based on Knowledge Graph)**

这部分主要涉及特征提取（VMD）、图谱边权重计算（故障-特征相关性）以及改进的随机森林算法。

### 1. VMD 模态能量特征 (公式 1)

文中通过变分模态分解（VMD）得到多个模态分量，计算其能量作为特征。

#### 核心公式
$$
E_m = \int_{-\infty}^{+\infty} |C_m|^2 dt
$$

*   **含义**：计算第 $m$ 个模态信号的能量。在离散数据中，这等同于该模态所有采样点数值的平方和。

#### 数值计算示例
假设 VMD 分解出的第 1 个模态 $C_1$ 包含 5 个采样点：
$C_1 = [1.0, -2.0, 0.5, -1.0, 2.0]$

**计算步骤**：
1.  对每个点求平方：
    *   $1.0^2 = 1.0$
    *   $(-2.0)^2 = 4.0$
    *   $0.5^2 = 0.25$
    *   $(-1.0)^2 = 1.0$
    *   $2.0^2 = 4.0$
2.  求和（积分的离散形式）：
    $$
    Sum = 1.0 + 4.0 + 0.25 + 1.0 + 4.0 = 10.25
    $$

**结果**：该模态的能量特征 $E_1 = 10.25$。

---

### 2. 故障-特征相关性计算 (公式 2, 3, 4)

这是构建知识图谱权重的核心。它衡量**第 $k$ 个特征**对于识别**第 $i$ 类故障**的重要性 $w_{ik}$。逻辑是倒序计算：先算中心，再算离散度，最后算权重。

#### 设定场景
*   **故障类型 ($i$)**：2 类，内圈故障 ($i=1$) 和 外圈故障 ($i=2$)。
*   **特征 ($k$)**：关注特征 1（例如均方根值）。
*   **数据集**：
    *   内圈故障 ($i=1$) 的 3 个样本，特征值：$[10, 12, 11]$。
    *   外圈故障 ($i=2$) 的 3 个样本，特征值：$[5, 25, 45]$ (波动极大)。

#### 步骤 1：计算数据中心 $v_{ik}$ (公式 4)
$$
v_{ik} = \frac{\sum_{j=1}^{N} u_{ij} x_{jk}}{\sum_{j=1}^{N} u_{ij}}
$$
*   **含义**：计算第 $i$ 类故障在第 $k$ 个特征上的平均值。$u_{ij}$ 是隶属度（属于该类为1，否则为0）。

**计算**：
*   内圈故障 ($i=1$)：
    $$
    v_{11} = \frac{10+12+11}{3} = 11
    $$
*   外圈故障 ($i=2$)：
    $$
    v_{21} = \frac{5+25+45}{3} = 25
    $$

#### 步骤 2：计算类内距离/离散度 $\sigma_{ik}$ (公式 3)
$$
\sigma_{ik} = \sum_{j=1}^{N} u_{ij} (x_{jk} - v_{ik})^2
$$
*   **含义**：计算数据点距离中心的平方和（类似方差）。**数值越小，说明特征对该故障越稳定**。

**计算**：
*   内圈故障 ($i=1$)，中心为 11：
    $$
    \sigma_{11} = (10-11)^2 + (12-11)^2 + (11-11)^2 = 1 + 1 + 0 = 2
    $$
*   外圈故障 ($i=2$)，中心为 25：
    $$
    \sigma_{21} = (5-25)^2 + (25-25)^2 + (45-25)^2 = 400 + 0 + 400 = 800
    $$

#### 步骤 3：计算相关性权重 $w_{ik}$ (公式 2)
$$
w_{ik} = \frac{(\sigma_{ik})^{-1/2}}{\sum_{k'=1}^{D} (\sigma_{ik'})^{-1/2}}
$$
*   **含义**：取离散度的倒数平方根。$\sigma$ 越小，权重 $w$ 越大。

**计算核心项 $(\sigma_{ik})^{-1/2}$**：
*   内圈故障 ($i=1$)：
    $$
    (\sigma_{11})^{-0.5} = \frac{1}{\sqrt{2}} \approx 0.707
    $$
*   外圈故障 ($i=2$)：
    $$
    (\sigma_{21})^{-0.5} = \frac{1}{\sqrt{800}} \approx 0.035
    $$

**结论**：特征 1 与内圈故障的相关性 (0.707) 远高于外圈故障 (0.035)。在知识图谱中，"特征1" 指向 "内圈故障" 的边权重很高。

---

### 3. 改进的随机森林分裂 (加权熵)

利用计算出的权重 $w_{ik}$ 来改进决策树的分裂准则。

#### 加权熵公式 (公式 5)
$$
WEnt(N) = \sum_{i=1}^{|y|} (1 - w_{ik})^\eta p_i \log_2 p_i
$$
*   $p_i$：节点中第 $i$ 类样本的概率。
*   $w_{ik}$：相关性权重。
*   $\eta$：调节参数（设为 1/3）。
*   **含义**：如果特征 $k$ 对故障 $i$ 重要（$w_{ik}$ 大），则 $(1-w_{ik})$ 接近 0，从而消除该类造成的“混乱度”。

#### 数值计算示例
一个节点有 10 个样本：5 个内圈故障 ($p_1=0.5$)，5 个外圈故障 ($p_2=0.5$)。
假设使用 **特征 1** 分裂，且 $w_{11}=0.9$ (高相关), $w_{21}=0.1$ (低相关)。设 $\eta=1$。

*   **第一项 (内圈)**：
    $$
    (1 - 0.9)^1 \times 0.5 \times \log_2 0.5 = 0.1 \times 0.5 \times (-1) = -0.05
    $$
*   **第二项 (外圈)**：
    $$
    (1 - 0.1)^1 \times 0.5 \times \log_2 0.5 = 0.9 \times 0.5 \times (-1) = -0.45
    $$
*   **总加权熵**：
    $$
    WEnt = -0.05 - 0.45 = -0.5
    $$
    *(相比传统熵 -1.0，该值绝对值更小，说明该特征被认为能有效降低不确定性)*

#### 信息增益比 (公式 6)
$$
Gain(N, k) = Ent(N) - \sum_{v=1}^{V} \frac{|N^v|}{|N|} WEnt(N^v)
$$
*   **含义**：用父节点的传统熵减去子节点的加权熵，选择增益最大的特征进行分裂。

---
---

# 第二部分：用于机器智能故障诊断的少样本迁移学习
**(Few-shot Transfer Learning for Intelligent Fault Diagnosis)**

这部分主要涉及信号预处理（FFT）、特征编码（1D-CNN）以及元学习中的关系网络（Relation Network）评分机制。

### 1. 信号预处理：快速傅里叶变换 (FFT)

文中将时域信号转换为频域信号作为输入。

#### 核心公式
$$
X[k] = \sum_{n=0}^{N-1} x[n] \cdot e^{-j \frac{2\pi}{N} k n}
$$

#### 数值计算示例
假设截断信号长度 $N=4$，输入 $x = [1, 2, 1, 2]$。

**计算 $k=0$ (直流分量)**：
$$
X[0] = 1 \cdot e^0 + 2 \cdot e^0 + 1 \cdot e^0 + 2 \cdot e^0 = 6
$$

**计算 $k=2$ (高频分量)**：
$$
X[2] = 1 + 2e^{-j\pi} + 1e^{-j2\pi} + 2e^{-j3\pi}
$$
$$
= 1 + 2(-1) + 1(1) + 2(-1) = 1 - 2 + 1 - 2 = -2
$$
取模值 $|X[2]| = 2$。

**网络输入**：取幅度谱的一半，例如 $[6, 0, 2]$。

---

### 2. 特征编码器：一维卷积 (1D-CNN)

用于从序列信号中提取深层特征。

#### 核心公式
$$
y[i] = \sum_{k=0}^{K-1} w[k] \cdot x[i+k] + b
$$

#### 数值计算示例
*   **输入**：$[10, 20, 10, 20, 30]$
*   **卷积核 ($K=3$)**：$[0.5, 0, -0.5]$
*   **偏置 $b$**：0

**计算第 1 个输出点** (对应窗口 $[10, 20, 10]$)：
$$
y[0] = (10 \times 0.5) + (20 \times 0) + (10 \times -0.5) = 5 + 0 - 5 = 0
$$

**计算第 3 个输出点** (对应窗口 $[10, 20, 30]$)：
$$
y[2] = (10 \times 0.5) + (20 \times 0) + (30 \times -0.5) = 5 + 0 - 15 = -10
$$

---

### 3. 关系网络 (Relation Network) 评分与损失

这是少样本学习的核心，计算两个样本的相似度。

#### A. 特征拼接 (Concatenation)
$$
z = \text{Concat}(f_{\theta}(x_s), f_{\theta}(x_q))
$$

**示例**：
*   支持集样本特征 $v_s = [0.8, 0.2]$
*   查询集样本特征 $v_q = [0.9, 0.1]$
*   拼接结果：
    $$
    z = [0.8, 0.2, 0.9, 0.1]
    $$

#### B. 关系评分 (Relation Score)
$$
\text{Score} = \sigma(W \cdot z + b)
$$
其中 $\sigma$ 是 Sigmoid 函数 $\frac{1}{1+e^{-x}}$。

**示例**：
设 $W=[1,1,1,1]$, $b=-2$，输入 $z$ 如上。
$$
Sum = 0.8+0.2+0.9+0.1 - 2 = 0
$$
$$
\text{Score} = \frac{1}{1+e^{0}} = 0.5
$$

#### C. 损失函数 (MSE Loss)
$$
L = \sum (y_{true} - y_{pred})^2
$$

**示例**：
*   若两样本同类 ($y_{true}=1$)，模型预测分数 0.3（判断错误）。
*   损失计算：
    $$
    L = (1 - 0.3)^2 = 0.49
    $$

---

### 4. 优化与预测目标

#### 元学习阶段 (Meta-learning Stage)
寻找最佳参数 $\theta$ 以最小化所有任务的损失：
$$
\theta^* = \arg\min_{\theta} \sum_{(x_s, x_q) \in D_{source}} \text{Loss}(\text{RelationNet}(x_s, x_q), \text{Label})
$$

#### 迁移/测试阶段 (Transfer Stage)
对于新样本，选择关系评分最高的类别：
$$
\text{Accuracy} = \arg\max_{c} \text{RelationScore}(x_{support}^c, x_{new})
$$

**示例**：
新样本 $x_{new}$ 与三个故障样本的评分分别为：0.2, 0.9, 0.4。
$$
\arg\max(0.2, 0.9, 0.4) \rightarrow \text{Fault B (0.9)}
$$
判定结果为故障 B。